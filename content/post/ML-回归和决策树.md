+++
title = "ML-决策树和回归"
draft = false
date = "2016-05-20"
Categories = ["促进大数据发展行动纲要"] 
Description = "" 
Tags = ["ml","math"] 
toc = true

+++



## 决策树
### 理论基础
- 信息熵：-p(x)*lnp(x)的和,如果信息熵高的话，说明不稳定，不确定性大。例如一个0.5vs0.5的硬币和一个0.99vs0.01的硬币。
- 条件熵：H(Y|X)=
- 信息增益：互信息
- 信息增益率
- 基尼系数（两个定义）
ID3,C4.5,CART


建立决策树，核心的问题是如何选择特征值作为根节点？
找到不确定性降低最快的，即构建一个信息熵下降最快的树

决策树的评价：

随机森林，对M个样本，是随机选出N个样本，并且放回，构造L颗树，然后投票选出分类，这是一种集中的思想，每一个树不完美，然后一起就很完美。（分类器不一定用决策树，也可能用SVM、LR之类，但是效果不好，因为是强分类器，对噪声比较敏感）
事实上，回归也可以使用集中这种精神。

也有可能是在特征上加入了随机性，只取某些特征。降低了某些特征的影响。

怎么确定树的个数？
这个是一个超参数，和lambda也一样，拍脑袋，然后验证。
很多实践中，N=M,因为有1-1/e的概率会重复，会去掉捣乱分子，然后防止过拟合。各个大学的特征脸，加完除以n之后就会把不太好的特征给抹掉

平方根大法，每次平方根个样本

### 决策树也可以用来拟合
即每一段当做一个连续值


投票机制，可以加权。
例如，电影加权评分调节冷门和热门比赛。
拉普拉斯平滑，例如中韩比赛前四次都输了，计算最新比赛的胜率，本来应该是0/4，但是为了平滑，按照1/4+1来算。

## 回归

### 数学理论
#### 中心极限定理
#### e的含义
a的x次方，只有a=e时候的导数是一致的，对应现实是利息，利息将时间考虑了进来。
### 回归分析

#### 目标函数
1. 假设误差是IID高斯分布，对真个样本空间来说，观察到所有样本的联合分布是所有的误差分布乘起来，对之求对数然后求导，得到均方误差公式，某种意义上来说，样本的因变量Y也是符合高斯分布的。
2. 更一般意义的，均方误差公式其实对应的是欧氏距离。
3. 均方误差公式可以写成矩阵相乘的形式，θX-Y
3. 扰动，就类似于θ的值非常大的话，函数在样本空间之外的空间就可能出现过拟合，所以要防止θ过大，增加一个lamdaI矩阵，这个lamda叫做超常数，可以是指定的，例如=0.0001,根据训练数据